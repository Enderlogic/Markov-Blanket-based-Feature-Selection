import collections
from copy import deepcopy
from itertools import chain
from numpy.linalg import inv
from numba import njit

import numpy
from bnsl.accessory import g_test, cor_test
from rpy2.robjects import pandas2ri
from rpy2.robjects.packages import importr
from skopt import gp_minimize, Space
from skopt.space import Real

pandas2ri.activate()
base, bnlearn, stats = importr('base'), importr('bnlearn'), importr('stats')


def chunks(lst, n):
    """Yield successive n-fold chunks from lst."""
    size = round(len(lst) / n)
    for i in range(0, len(lst), size):
        yield lst[i:i + size]


@njit(fastmath=True)
def krr_numpy(X_train, y_train, X_test, lamda, sigma):
    X = numpy.concatenate((X_train, X_test)).transpose()
    X_amend = numpy.zeros((X.shape[0], X.shape[1], X.shape[1]))
    for i in range(X.shape[0]):
        for j in range(X.shape[1]):
            for k in range(X.shape[1]):
                X_amend[i, j, k] = X[i, j]
    K = X_amend - numpy.transpose(X_amend, (0, 2, 1))
    K = numpy.power(K, 2).sum(axis=0)
    K = numpy.exp(-K / sigma ** 2 / 2)
    y_pred = numpy.zeros(X_test.shape[0])
    M = inv(K[: X_train.shape[0], :][:, : X_train.shape[0]] + lamda * numpy.identity(X_train.shape[0]))
    KM = numpy.zeros((X_test.shape[0], X_train.shape[0]))
    for i in range(X_test.shape[0]):
        for j in range(X_train.shape[0]):
            for k in range(X_train.shape[0]):
                KM[i, j] += K[i + X_train.shape[0], k] * M[k, j]
    for i in range(X_test.shape[0]):
        for j in range(X_train.shape[0]):
            y_pred[i] += KM[i, j] * y_train[j]
    return y_pred


def krr_iterative_imputation(data, prune='complete', k=5, threshold=0.1, max_iteration=10, lamda=0.1, sigma=20, bo=True,
                             sigma_low=1, sigma_high=30, lamda_low=0.01, lamda_high=0.4, n_calls=20,
                             n_initial_points=10, n_points=1000):
    """
    @param data: input data_bn that might contain missing values
    @param prune: whether to prune unrelated variables as the dependent variable of the target variable
    @param k: number of sub-data_bn in cross validation
    @param threshold: threshold of conditional independence test
    @param max_iteration: maximum number of iterations to be performed before the stopping criterion is met
    @param lamda: penalty parameter used for KRR
    @param sigma: kernel variance of RBF kernel
    @param bo: whether to use Bayesian Optimization
    @param sigma_low: the lowest value of sigma searched by Bayesian Optimization
    @param sigma_high: the highest value of sigma searched by Bayesian Optimization
    @param lamda_low: the lowest value of lamda searched by Bayesian Optimization
    @param lamda_high: the highest value of lamda searched by Bayesian Optimization
    @param n_calls: maximum number of calls for Bayesian Optimization
    @param n_initial_points: number of initial points generated by Bayesian Optimization
    @param n_points: number of points searched by Bayesian Optimization in one step
    @return: imputed data_bn
    """
    data_imputed = data.fillna(data.mean())
    # var_missing = data_bn.columns[data_bn.isnull().any()].tolist()
    var_missing = data.isnull().sum(axis=0).sort_values()
    var_missing = var_missing[var_missing > 0]
    var_missing = var_missing.index.to_list()
    missing_idx = {
        var: {'complete': data[~data[var].isnull()].index.tolist(), 'missing': data[data[var].isnull()].index.tolist()}
        for var in var_missing}
    sigma_dict = {var: 1 for var in var_missing}
    lamda_dict = {var: 0.1 for var in var_missing}
    predictor_dict = {var: data.columns[data.columns != var].to_list() for var in var_missing}
    if prune == 'partial':
        if all(data.dtypes == 'category'):
            ci_test = g_test
            data_numpy = data.apply(lambda x: x.cat.codes).to_numpy()
        elif all(data.dtypes != 'category'):
            ci_test = cor_test
            data_numpy = data.to_numpy()
        else:
            raise Exception('Mixed type of data_bn is not supported.')
        varnames = list(data.columns)
        cache = {}
        for var in var_missing:
            # forward
            predictor_dict[var] = []
            candidate = {v: 0 for v in data.columns if v != var}
            while True:
                for can in candidate:
                    if tuple(sorted([var, can])) not in cache:
                        cache[tuple(sorted([var, can]))] = {}
                    if tuple(sorted(predictor_dict[var])) not in cache[tuple(sorted([var, can]))]:
                        cols = [varnames.index(x) for x in [var, can] + predictor_dict[var]]
                        cache[tuple(sorted([var, can]))][tuple(sorted(predictor_dict[var]))] = ci_test(
                            data_numpy[~numpy.isnan(data_numpy[:, cols]).any(axis=1), :], cols) if sum(
                            ~numpy.isnan(data_numpy[:, cols]).any(axis=1)) > 2 else 0
                    candidate[can] = cache[tuple(sorted([var, can]))][tuple(sorted(predictor_dict[var]))]
                if min(candidate.values()) < threshold:
                    predictor_dict[var].append(min(candidate, key=candidate.get))
                    del candidate[predictor_dict[var][-1]]
                    if len(candidate) == 0:
                        break
                else:
                    break
            # backward
            candidate = {v: 0 for v in predictor_dict[var][: -1]}
            for can in candidate:
                con = [x for x in predictor_dict[var] if x != can]
                if tuple(sorted(con)) not in cache[tuple(sorted([var, can]))]:
                    cols = [varnames.index(x) for x in [var, can] + con]
                    cache[tuple(sorted([var, can]))][tuple(sorted(con))] = ci_test(data_numpy, cols) if sum(
                        ~numpy.isnan(data_numpy[:, cols]).any(axis=1)) > 2 else 0
                if cache[tuple(sorted([var, can]))][tuple(sorted(con))] >= threshold:
                    predictor_dict[var].remove(can)
    elif prune == 'complete':
        learned = bnlearn.pc_stable(data, alpha=threshold)
        for var in var_missing:
            predictor_dict[var] = list(bnlearn.mb(learned, var))
    elif prune != 'None':
        raise Exception(prune + ' pruning method is undefined.')
    for var in var_missing:
        data_train = data_imputed.loc[~data[var].isnull()].reset_index(drop=True)
        data_test = data_imputed.loc[data[var].isnull()].reset_index(drop=True)
        idx_chunks = list(chunks(list(range(len(data_train))), k))

        def cv_mse(args):
            sigma = args[0]
            lamda = args[1]
            mse = 0
            for i in range(k):
                test_id = idx_chunks[i]
                train_id = list(chain.from_iterable([x for j, x in enumerate(idx_chunks) if j != i]))
                imputed = krr_numpy(data_train.loc[train_id, predictor_dict[var]].to_numpy(),
                                    data_train.loc[train_id, var].to_numpy(),
                                    data_train.loc[test_id, predictor_dict[var]].to_numpy(), lamda, sigma)
                mse += data_train.loc[test_id, var].sub(imputed).pow(2).sum() / len(test_id) / k
            return mse

        if bo:
            # find the optimal hyperparameters for KRR via Bayesian Optimization
            [sigma_dict[var], lamda_dict[var]] = gp_minimize(cv_mse, Space(
                [Real(name='sigma', low=sigma_low, high=sigma_high),
                 Real(name='lamda', low=lamda_low, high=lamda_high)]), n_calls=n_calls,
                                                             n_initial_points=n_initial_points, n_points=n_points).x
        else:
            sigma_dict[var] = sigma
            lamda_dict[var] = lamda
        data_imputed.loc[missing_idx[var]['missing'], var] = krr_numpy(data_train[predictor_dict[var]].to_numpy(),
                                                                       data_train[var].to_numpy(),
                                                                       data_test[predictor_dict[var]].to_numpy(),
                                                                       lamda_dict[var], sigma_dict[var])
        # print('lamda', lamda_dict[var])
        # print('sigma', sigma_dict[var])
    diff = numpy.Inf
    i = 1
    while True:
        data_cache = deepcopy(data_imputed)
        for var in var_missing:
            # impute missing value by KRR
            data_imputed.loc[missing_idx[var]['missing'], var] = krr_numpy(
                data_imputed.loc[~data[var].isnull(), predictor_dict[var]].to_numpy(),
                data_imputed.loc[~data[var].isnull(), var].to_numpy(),
                data_imputed.loc[data[var].isnull(), predictor_dict[var]].to_numpy(), lamda_dict[var], sigma_dict[var])
        if data_cache.sub(data_imputed).pow(2).sum().sum() > diff:
            break
        else:
            diff = data_cache.sub(data_imputed).pow(2).sum().sum()
        i += 1
        if i > max_iteration:
            break
    return data_imputed


def rmse(data_clean, data_imputed, data_missing):
    data_imputed = (data_imputed - data_clean.min()) / (data_clean.max() - data_clean.min())
    data_true = (data_clean - data_clean.min()) / (data_clean.max() - data_clean.min())
    return numpy.sqrt(data_imputed.sub(data_true).pow(2).sum().sum() / data_missing.isna().sum().sum())


def pfc(data_true, data_imputed, data_missing):
    var_missing = data_missing.columns[data_missing.isnull().any()]
    pfc_value = 0
    varnames = data_true.columns.to_list()
    for var in var_missing:
        pfc_value += sum(data_true.values[:, varnames.index(var)] != data_imputed.values[:, varnames.index(var)]) / \
                     data_missing[var].isnull().sum()
    return pfc_value / len(var_missing)
